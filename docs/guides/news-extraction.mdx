---
title: 'News Extraction Use Cases'
description: 'Learn how to extract and monitor news content using WebLinq API'
---

# News Extraction Use Cases

Learn how to extract, monitor, and analyze news content at scale using WebLinq API.

## Overview

<Note>This guide demonstrates how to implement common news extraction use cases using WebLinq API.</Note>

## Article Extraction

### Basic Article Scraping

<CodeGroup>
```javascript Node.js
const WebLinq = require('weblinq');

class ArticleScraper {
constructor(apiKey) {
this.client = new WebLinq(apiKey);
}

async scrapeArticle(url) {
const response = await this.client.extract({
url,
selectors: {
title: {
selector: 'h1, .article-title',
type: 'text'
},
author: {
selector: '.author-name, .byline',
type: 'text'
},
publishDate: {
selector: '.publish-date, time',
attr: 'datetime'
},
content: {
selector: '.article-content, .story-content',
type: 'html'
},
summary: {
selector: '.article-summary, .story-description',
type: 'text'
},
categories: {
selector: '.article-categories a, .tags a',
type: 'list'
},
images: {
selector: '.article-content img',
type: 'list',
attr: 'src'
}
}
});

    return {
      ...response,
      url,
      extractedAt: new Date().toISOString()
    };

}

cleanContent(content) {
// Remove ads, social media embeds, etc.
return content
.replace(/<div class="ad">._?<\/div>/g, '')
.replace(/<blockquote class="twitter-tweet">._?<\/blockquote>/g, '')
.trim();
}
}

// Usage
const scraper = new ArticleScraper('YOUR_API_KEY');

const article = await scraper.scrapeArticle('https://news-site.com/article');
console.log(article);

````

```python Python
from weblinq import WebLinq
from datetime import datetime
from typing import Dict, List

class ArticleScraper:
    def __init__(self, api_key: str):
        self.client = WebLinq(api_key)

    async def scrape_article(self, url: str) -> dict:
        response = await self.client.extract({
            'url': url,
            'selectors': {
                'title': {
                    'selector': 'h1, .article-title',
                    'type': 'text'
                },
                'author': {
                    'selector': '.author-name, .byline',
                    'type': 'text'
                },
                'publish_date': {
                    'selector': '.publish-date, time',
                    'attr': 'datetime'
                },
                'content': {
                    'selector': '.article-content, .story-content',
                    'type': 'html'
                },
                'summary': {
                    'selector': '.article-summary, .story-description',
                    'type': 'text'
                },
                'categories': {
                    'selector': '.article-categories a, .tags a',
                    'type': 'list'
                },
                'images': {
                    'selector': '.article-content img',
                    'type': 'list',
                    'attr': 'src'
                }
            }
        })

        return {
            **response,
            'url': url,
            'extracted_at': datetime.now().isoformat()
        }

    def clean_content(self, content: str) -> str:
        import re

        # Remove ads, social media embeds, etc.
        content = re.sub(r'<div class="ad">.*?</div>', '', content)
        content = re.sub(r'<blockquote class="twitter-tweet">.*?</blockquote>', '', content)
        return content.strip()

# Usage
async def main():
    scraper = ArticleScraper('YOUR_API_KEY')
    article = await scraper.scrape_article('https://news-site.com/article')
    print(article)

asyncio.run(main())
````

</CodeGroup>

### News Feed Monitoring

<CodeGroup>
```javascript Node.js
class NewsFeedMonitor {
  constructor(apiKey) {
    this.client = new WebLinq(apiKey);
    this.feeds = new Map();
    this.scraper = new ArticleScraper(apiKey);
  }

async addFeed(url, options = {}) {
const { name, interval = 3600000 } = options;

    this.feeds.set(url, {
      name,
      interval,
      lastCheck: null,
      articles: new Set()
    });

}

async monitorFeed(url) {
const feed = this.feeds.get(url);

    try {
      const response = await this.client.extract({
        url,
        selectors: {
          articles: {
            selector: '.article-item, .story-card',
            type: 'list',
            data: {
              title: '.article-title',
              url: {
                selector: 'a',
                attr: 'href'
              },
              summary: '.article-summary'
            }
          }
        }
      });

      for (const article of response.articles) {
        if (!feed.articles.has(article.url)) {
          await this.processNewArticle(article, feed);
        }
      }

      feed.lastCheck = new Date();
    } catch (error) {
      console.error(`Error monitoring feed ${url}:`, error);
    }

}

async processNewArticle(article, feed) {
try {
const fullArticle = await this.scraper.scrapeArticle(article.url);
feed.articles.add(article.url);

      await this.storeArticle(fullArticle);
      await this.notifyNewArticle(fullArticle, feed);
    } catch (error) {
      console.error(`Error processing article ${article.url}:`, error);
    }

}

async storeArticle(article) {
// Implement your storage logic
console.log('Storing article:', article.title);
}

async notifyNewArticle(article, feed) {
// Implement your notification logic
console.log(`New article in ${feed.name}:`, article.title);
}

startMonitoring() {
for (const [url, feed] of this.feeds) {
setInterval(() => this.monitorFeed(url), feed.interval);
}
}
}

// Usage
const monitor = new NewsFeedMonitor('YOUR_API_KEY');

await monitor.addFeed('https://news-site.com/feed', {
name: 'Tech News',
interval: 1800000 // 30 minutes
});

monitor.startMonitoring();

````

```python Python
class NewsFeedMonitor:
    def __init__(self, api_key: str):
        self.client = WebLinq(api_key)
        self.feeds: Dict[str, dict] = {}
        self.scraper = ArticleScraper(api_key)

    async def add_feed(self, url: str, **options):
        name = options.get('name', url)
        interval = options.get('interval', 3600)  # 1 hour default

        self.feeds[url] = {
            'name': name,
            'interval': interval,
            'last_check': None,
            'articles': set()
        }

    async def monitor_feed(self, url: str):
        feed = self.feeds[url]

        try:
            response = await self.client.extract({
                'url': url,
                'selectors': {
                    'articles': {
                        'selector': '.article-item, .story-card',
                        'type': 'list',
                        'data': {
                            'title': '.article-title',
                            'url': {
                                'selector': 'a',
                                'attr': 'href'
                            },
                            'summary': '.article-summary'
                        }
                    }
                }
            })

            for article in response['articles']:
                if article['url'] not in feed['articles']:
                    await self.process_new_article(article, feed)

            feed['last_check'] = datetime.now()
        except Exception as e:
            print(f"Error monitoring feed {url}: {e}")

    async def process_new_article(self, article: dict, feed: dict):
        try:
            full_article = await self.scraper.scrape_article(article['url'])
            feed['articles'].add(article['url'])

            await self.store_article(full_article)
            await self.notify_new_article(full_article, feed)
        except Exception as e:
            print(f"Error processing article {article['url']}: {e}")

    async def store_article(self, article: dict):
        # Implement your storage logic
        print('Storing article:', article['title'])

    async def notify_new_article(self, article: dict, feed: dict):
        # Implement your notification logic
        print(f"New article in {feed['name']}:", article['title'])

    async def start_monitoring(self):
        while True:
            tasks = []
            for url in self.feeds:
                tasks.append(self.monitor_feed(url))

            await asyncio.gather(*tasks)
            await asyncio.sleep(min(
                feed['interval']
                for feed in self.feeds.values()
            ))

# Usage
async def main():
    monitor = NewsFeedMonitor('YOUR_API_KEY')

    await monitor.add_feed('https://news-site.com/feed',
                          name='Tech News',
                          interval=1800)  # 30 minutes

    await monitor.start_monitoring()

asyncio.run(main())
````

</CodeGroup>

## Content Analysis

### Sentiment Analysis

<CodeGroup>
```javascript Node.js
class ContentAnalyzer {
  constructor(apiKey) {
    this.client = new WebLinq(apiKey);
  }

async analyzeArticle(url) {
const article = await this.client.extract({
url,
selectors: {
title: 'h1',
content: '.article-content',
summary: '.article-summary'
}
});

    const analysis = await this.analyzeSentiment(article);
    const entities = await this.extractEntities(article);
    const topics = await this.classifyTopics(article);

    return {
      url,
      article,
      analysis: {
        sentiment: analysis,
        entities,
        topics
      },
      timestamp: new Date().toISOString()
    };

}

async analyzeSentiment(article) {
// Implement sentiment analysis
// You can use services like Google Cloud Natural Language API
return {
score: 0.8, // -1 to 1
magnitude: 0.6
};
}

async extractEntities(article) {
// Extract named entities
return {
organizations: [],
people: [],
locations: []
};
}

async classifyTopics(article) {
// Classify article topics
return ['technology', 'ai', 'business'];
}
}

// Usage
const analyzer = new ContentAnalyzer('YOUR_API_KEY');
const analysis = await analyzer.analyzeArticle('https://news-site.com/article');

````

```python Python
from typing import Dict, List
import json

class ContentAnalyzer:
    def __init__(self, api_key: str):
        self.client = WebLinq(api_key)

    async def analyze_article(self, url: str) -> dict:
        article = await self.client.extract({
            'url': url,
            'selectors': {
                'title': 'h1',
                'content': '.article-content',
                'summary': '.article-summary'
            }
        })

        analysis = await self.analyze_sentiment(article)
        entities = await self.extract_entities(article)
        topics = await self.classify_topics(article)

        return {
            'url': url,
            'article': article,
            'analysis': {
                'sentiment': analysis,
                'entities': entities,
                'topics': topics
            },
            'timestamp': datetime.now().isoformat()
        }

    async def analyze_sentiment(self, article: dict) -> dict:
        # Implement sentiment analysis
        # You can use services like Google Cloud Natural Language API
        return {
            'score': 0.8,  # -1 to 1
            'magnitude': 0.6
        }

    async def extract_entities(self, article: dict) -> dict:
        # Extract named entities
        return {
            'organizations': [],
            'people': [],
            'locations': []
        }

    async def classify_topics(self, article: dict) -> List[str]:
        # Classify article topics
        return ['technology', 'ai', 'business']

# Usage
async def main():
    analyzer = ContentAnalyzer('YOUR_API_KEY')
    analysis = await analyzer.analyze_article('https://news-site.com/article')
    print(json.dumps(analysis, indent=2))

asyncio.run(main())
````

</CodeGroup>

### Trend Analysis

<CodeGroup>
```javascript Node.js
class TrendAnalyzer {
  constructor(apiKey) {
    this.client = new WebLinq(apiKey);
    this.trends = new Map();
  }

async trackTrend(keyword, sources) {
const articles = await this.searchArticles(keyword, sources);
const analysis = await this.analyzeTrend(keyword, articles);

    this.trends.set(keyword, {
      keyword,
      analysis,
      timestamp: new Date().toISOString()
    });

    return analysis;

}

async searchArticles(keyword, sources) {
const articles = [];

    for (const source of sources) {
      const response = await this.client.extract({
        url: source,
        selectors: {
          articles: {
            selector: '.article-item',
            type: 'list',
            data: {
              title: '.article-title',
              date: '.article-date',
              url: {
                selector: 'a',
                attr: 'href'
              }
            }
          }
        }
      });

      articles.push(...response.articles.filter(article =>
        article.title.toLowerCase().includes(keyword.toLowerCase())
      ));
    }

    return articles;

}

async analyzeTrend(keyword, articles) {
const timeline = this.buildTimeline(articles);
const sentiment = await this.analyzeSentiment(articles);
const relatedTerms = this.findRelatedTerms(articles);

    return {
      keyword,
      articleCount: articles.length,
      timeline,
      sentiment,
      relatedTerms
    };

}

buildTimeline(articles) {
const timeline = new Map();

    for (const article of articles) {
      const date = new Date(article.date).toISOString().split('T')[0];
      timeline.set(date, (timeline.get(date) || 0) + 1);
    }

    return Object.fromEntries(timeline);

}

findRelatedTerms(articles) {
const terms = new Map();

    for (const article of articles) {
      const words = article.title.toLowerCase().split(/\W+/);

      for (const word of words) {
        if (word.length > 3) {  // Skip short words
          terms.set(word, (terms.get(word) || 0) + 1);
        }
      }
    }

    return Array.from(terms.entries())
      .sort((a, b) => b[1] - a[1])
      .slice(0, 10)
      .map(([term, count]) => ({ term, count }));

}
}

// Usage
const analyzer = new TrendAnalyzer('YOUR_API_KEY');

const sources = [
'https://news-site.com/tech',
'https://another-news.com/technology'
];

const trend = await analyzer.trackTrend('artificial intelligence', sources);
console.log(trend);

````

```python Python
from collections import defaultdict
from datetime import datetime
from typing import Dict, List

class TrendAnalyzer:
    def __init__(self, api_key: str):
        self.client = WebLinq(api_key)
        self.trends: Dict[str, dict] = {}

    async def track_trend(self, keyword: str, sources: List[str]) -> dict:
        articles = await self.search_articles(keyword, sources)
        analysis = await self.analyze_trend(keyword, articles)

        self.trends[keyword] = {
            'keyword': keyword,
            'analysis': analysis,
            'timestamp': datetime.now().isoformat()
        }

        return analysis

    async def search_articles(self, keyword: str, sources: List[str]) -> List[dict]:
        articles = []

        for source in sources:
            response = await self.client.extract({
                'url': source,
                'selectors': {
                    'articles': {
                        'selector': '.article-item',
                        'type': 'list',
                        'data': {
                            'title': '.article-title',
                            'date': '.article-date',
                            'url': {
                                'selector': 'a',
                                'attr': 'href'
                            }
                        }
                    }
                }
            })

            articles.extend([
                article for article in response['articles']
                if keyword.lower() in article['title'].lower()
            ])

        return articles

    async def analyze_trend(self, keyword: str, articles: List[dict]) -> dict:
        timeline = self.build_timeline(articles)
        sentiment = await self.analyze_sentiment(articles)
        related_terms = self.find_related_terms(articles)

        return {
            'keyword': keyword,
            'article_count': len(articles),
            'timeline': timeline,
            'sentiment': sentiment,
            'related_terms': related_terms
        }

    def build_timeline(self, articles: List[dict]) -> Dict[str, int]:
        timeline = defaultdict(int)

        for article in articles:
            date = datetime.fromisoformat(article['date']).date().isoformat()
            timeline[date] += 1

        return dict(timeline)

    def find_related_terms(self, articles: List[dict]) -> List[dict]:
        from collections import Counter
        import re

        terms = Counter()

        for article in articles:
            words = re.findall(r'\w+', article['title'].lower())
            terms.update(word for word in words if len(word) > 3)

        return [
            {'term': term, 'count': count}
            for term, count in terms.most_common(10)
        ]

# Usage
async def main():
    analyzer = TrendAnalyzer('YOUR_API_KEY')

    sources = [
        'https://news-site.com/tech',
        'https://another-news.com/technology'
    ]

    trend = await analyzer.track_trend('artificial intelligence', sources)
    print(json.dumps(trend, indent=2))

asyncio.run(main())
````

</CodeGroup>

## Best Practices

### Content Deduplication

```javascript
class ContentDeduplicator {
  constructor() {
    this.articles = new Map();
  }

  async isDuplicate(article) {
    const hash = this.hashContent(article.content);
    const similarity = this.findSimilarContent(article.content);

    if (this.articles.has(hash) || similarity > 0.8) {
      return true;
    }

    this.articles.set(hash, {
      url: article.url,
      timestamp: Date.now(),
    });

    return false;
  }

  hashContent(content) {
    return crypto.createHash('sha256').update(this.normalizeContent(content)).digest('hex');
  }

  normalizeContent(content) {
    return content.toLowerCase().replace(/\s+/g, ' ').trim();
  }

  findSimilarContent(content) {
    // Implement fuzzy matching or similarity scoring
    return 0;
  }
}
```

### Error Handling

```javascript
class NewsScraperError extends Error {
  constructor(message, type, url) {
    super(message);
    this.name = 'NewsScraperError';
    this.type = type;
    this.url = url;
  }
}

class ErrorHandler {
  constructor() {
    this.errors = [];
  }

  async withRetry(fn, options = {}) {
    const { retries = 3, delay = 1000 } = options;

    for (let i = 0; i < retries; i++) {
      try {
        return await fn();
      } catch (error) {
        this.logError(error);

        if (i === retries - 1) throw error;

        await new Promise((resolve) => setTimeout(resolve, delay * Math.pow(2, i)));
      }
    }
  }

  logError(error) {
    this.errors.push({
      timestamp: new Date().toISOString(),
      error: error.message,
      type: error.type,
      url: error.url,
    });
  }
}
```

## Summary

<Steps>
  <Step title="Extract Content">Implement article scraping and feed monitoring</Step>

  <Step title="Analyze Content">Add sentiment analysis and trend tracking</Step>

  <Step title="Handle Duplicates">Implement content deduplication</Step>

  <Step title="Manage Errors">Add proper error handling and retries</Step>
</Steps>

<Note>Need help implementing news extraction? Contact our [support team](/support/contact) for assistance.</Note>{' '}
